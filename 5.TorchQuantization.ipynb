{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/venom/intel/oneapi/intelpython/latest/envs/pytorch-gpu/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/venom/intel/oneapi/intelpython/latest/envs/pytorch-gpu/lib/python3.9/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: /home/venom/intel/oneapi/intelpython/python3.9/envs/pytorch-gpu/lib/python3.9/site-packages/torchvision/image.so: undefined symbol: _ZN5torch3jit17parseSchemaOrNameERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEE\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    }
   ],
   "source": [
    "import utils\n",
    "import torch\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = utils.give_model()\n",
    "model.load_state_dict(torch.load(\"./model/model.pt\", map_location=\"cpu\"))\n",
    "train_dataloader, valid_dataloader, test_dataloader = utils.give_data_loaders()\n",
    "batch = next(iter(test_dataloader))\n",
    "criterion = utils.give_criterion()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "model.cpu()\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, val_loader, criterion):\n",
    "    total_loss = 0.0\n",
    "\n",
    "    with torch.no_grad(), tqdm(total=len(val_loader), desc=\"Validation\") as progress_bar:\n",
    "        for batch in val_loader:\n",
    "            images = batch[\"image\"]\n",
    "            masks = batch[\"mask\"]\n",
    "\n",
    "            logits_mask = model(images)\n",
    "            loss = criterion(logits_mask, masks)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            progress_bar.update(1)\n",
    "\n",
    "    return total_loss / len(val_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = [((torch.randn(1, 3, 256, 256), ), torch.tensor([0])) for _ in range(100)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-07-12 20:37:06\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mRunning Speedster on CPU\u001b[0m\n",
      "\u001b[32m2023-07-12 20:37:07\u001b[0m | \u001b[38;2;211;211;211mWARNING \u001b[0m | \u001b[38;2;211;211;211mMissing Frameworks: tensorflow.\n",
      " Please install them to include them in the optimization pipeline.\u001b[0m\n",
      "\u001b[32m2023-07-12 20:37:07\u001b[0m | \u001b[38;2;211;211;211mWARNING \u001b[0m | \u001b[38;2;211;211;211mMissing Compilers: onnxruntime, deepsparse.\n",
      " Please install them to include them in the optimization pipeline.\u001b[0m\n",
      "\u001b[32m2023-07-12 20:37:12\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mBenchmark performance of original model\u001b[0m\n",
      "\u001b[32m2023-07-12 20:37:18\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOriginal model latency: 0.042703731060028075 sec/iter\u001b[0m\n",
      "\u001b[32m2023-07-12 20:37:18\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1m[1/2] Running PyTorch Optimization Pipeline\u001b[0m\n",
      "\u001b[32m2023-07-12 20:37:18\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimizing with TorchScriptCompiler and q_type: None.\u001b[0m\n",
      "\u001b[32m2023-07-12 20:37:19\u001b[0m | \u001b[38;2;211;211;211mWARNING \u001b[0m | \u001b[38;2;211;211;211mUnable to trace model with torch.fx\u001b[0m\n",
      "\u001b[32m2023-07-12 20:37:21\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimized model latency: 0.03694915771484375 sec/iter\u001b[0m\n",
      "\u001b[32m2023-07-12 20:37:21\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1m[2/2] Running ONNX Optimization Pipeline\u001b[0m\n",
      "\u001b[32m2023-07-12 20:37:21\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimizing with OpenVINOCompiler and q_type: None.\u001b[0m\n",
      "[ INFO ] The model was converted to IR v11, the latest model format that corresponds to the source DL framework input/output format. While IR v11 is backwards compatible with OpenVINO Inference Engine API v1.0, please use API v2.0 (as of 2022.1) to take advantage of the latest improvements in IR v11.\n",
      "Find more information about API v2.0 and IR v11 at https://docs.openvino.ai/2023.0/openvino_2_0_transition_guide.html\n",
      "[ SUCCESS ] Generated IR version 11 model.\n",
      "[ SUCCESS ] XML file: /tmp/tmp7s6qzku2/fp32/temp.xml\n",
      "[ SUCCESS ] BIN file: /tmp/tmp7s6qzku2/fp32/temp.bin\n",
      "\u001b[32m2023-07-12 20:37:26\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimized model latency: 0.022192955017089844 sec/iter\u001b[0m\n",
      "\u001b[32m2023-07-12 20:37:26\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimizing with OpenVINOCompiler and q_type: QuantizationType.HALF.\u001b[0m\n",
      "[ INFO ] Generated IR will be compressed to FP16. If you get lower accuracy, please consider disabling compression by removing argument --compress_to_fp16 or set it to false --compress_to_fp16=False.\n",
      "Find more information about compression to FP16 at https://docs.openvino.ai/2023.0/openvino_docs_MO_DG_FP16_Compression.html\n",
      "[ INFO ] The model was converted to IR v11, the latest model format that corresponds to the source DL framework input/output format. While IR v11 is backwards compatible with OpenVINO Inference Engine API v1.0, please use API v2.0 (as of 2022.1) to take advantage of the latest improvements in IR v11.\n",
      "Find more information about API v2.0 and IR v11 at https://docs.openvino.ai/2023.0/openvino_2_0_transition_guide.html\n",
      "[ SUCCESS ] Generated IR version 11 model.\n",
      "[ SUCCESS ] XML file: /tmp/tmp7s6qzku2/fp32/temp.xml\n",
      "[ SUCCESS ] BIN file: /tmp/tmp7s6qzku2/fp32/temp.bin\n",
      "\u001b[32m2023-07-12 20:37:31\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimized model latency: 0.02132272720336914 sec/iter\u001b[0m\n",
      "\u001b[1m\n",
      "[Speedster results on 11th Gen Intel(R) Core(TM) i7-11800H @ 2.30GHz]\n",
      "┏━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
      "┃ Metric      ┃ Original Model   ┃ Optimized Model   ┃ Improvement   ┃\n",
      "┣━━━━━━━━━━━━━╋━━━━━━━━━━━━━━━━━━╋━━━━━━━━━━━━━━━━━━━╋━━━━━━━━━━━━━━━┫\n",
      "┃ backend     ┃ PYTORCH          ┃ OpenVINO          ┃               ┃\n",
      "┃ latency     ┃ 0.0427 sec/batch ┃ 0.0213 sec/batch  ┃ 2.00x         ┃\n",
      "┃ throughput  ┃ 23.42 data/sec   ┃ 46.90 data/sec    ┃ 2.00x         ┃\n",
      "┃ model size  ┃ 92.62 MB         ┃ 92.84 MB          ┃ 0%            ┃\n",
      "┃ metric drop ┃                  ┃ 0.0004            ┃               ┃\n",
      "┃ techniques  ┃                  ┃ fp16              ┃               ┃\n",
      "┗━━━━━━━━━━━━━┻━━━━━━━━━━━━━━━━━━┻━━━━━━━━━━━━━━━━━━━┻━━━━━━━━━━━━━━━┛\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from speedster import optimize_model\n",
    "\n",
    "# Run Speedster optimization\n",
    "optimized_model = optimize_model(\n",
    "    model,\n",
    "    input_data=input_data,\n",
    "    device='cpu',\n",
    "    store_latencies=True,\n",
    "    optimization_time=\"unconstrained\"\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
